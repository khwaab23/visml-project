{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a246c0",
   "metadata": {},
   "source": [
    "In cell 3 change the directory to the one you extracted the results in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afe2f2",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8700370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install geopandas shapely rtree requests fiona pyproj folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import math\n",
    "from pathlib import Path\n",
    "from shapely.geometry import LineString, box, Point, Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d7bb3",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7046ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUFFER_DISTANCE = 3  # meters - buffer for network segment matching within polygons\n",
    "ZOOM_LEVEL = 19  # from structure.json\n",
    "\n",
    "# Path to the data directory containing network/, polygons/, tiles/, and structure.json\n",
    "DATA_DIR = Path('./example')  # Update this path to your data directory\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Exists: {DATA_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b37194",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data directory exists\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found: {DATA_DIR}\")\n",
    "\n",
    "extract_dir = DATA_DIR\n",
    "\n",
    "print(f\"✓ Using data directory: {extract_dir}\")\n",
    "print(\"\\nContents:\")\n",
    "for item in sorted(extract_dir.rglob('*')):\n",
    "    if item.is_file() and (item.suffix in ['.json', '.csv', '.shp', '.dbf', '.shx', '.prj']):\n",
    "        print(f\"  {item.relative_to(extract_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb958cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load structure.json\n",
    "structure_file = list(extract_dir.rglob('structure.json'))[0]\n",
    "with open(structure_file, 'r') as f:\n",
    "    structure = json.load(f)\n",
    "\n",
    "print(\"Structure configuration:\")\n",
    "print(json.dumps(structure, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load tile info\n",
    "tile_info_files = list(extract_dir.rglob('*_info.json'))\n",
    "if not tile_info_files:\n",
    "    # Load from CSV\n",
    "    csv_files = list(extract_dir.rglob('*_info.csv'))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No tile info file found (neither JSON nor CSV)\")\n",
    "    \n",
    "    tiles_df = pd.read_csv(csv_files[0])\n",
    "    \n",
    "    # Convert CSV to dict format using correct column names\n",
    "    tile_info = {\n",
    "        'center': [\n",
    "            (tiles_df['topleft_y'].min() + tiles_df['bottomright_y'].max()) / 2,  # lat\n",
    "            (tiles_df['topleft_x'].min() + tiles_df['bottomright_x'].max()) / 2   # lon\n",
    "        ],\n",
    "        'zoom': ZOOM_LEVEL,\n",
    "        'bbox': [\n",
    "            tiles_df['bottomright_y'].min(),  # min_lat\n",
    "            tiles_df['bottomright_x'].min(),  # min_lon\n",
    "            tiles_df['topleft_y'].max(),      # max_lat\n",
    "            tiles_df['topleft_x'].max()       # max_lon\n",
    "        ]\n",
    "    }\n",
    "else:\n",
    "    # Load from JSON\n",
    "    with open(tile_info_files[0], 'r') as f:\n",
    "        tile_info_json = json.load(f)\n",
    "    \n",
    "    # Calculate center from bbox\n",
    "    bbox = tile_info_json['bbox']\n",
    "    tile_info = {\n",
    "        'center': [\n",
    "            (bbox[0] + bbox[1]) / 2,  # lat (average of min and max lat)\n",
    "            (bbox[2] + bbox[3]) / 2   # lon (average of min and max lon)\n",
    "        ],\n",
    "        'zoom': tile_info_json.get('zoom', ZOOM_LEVEL),\n",
    "        'bbox': [\n",
    "            bbox[0],  # min_lat\n",
    "            bbox[2],  # min_lon\n",
    "            bbox[1],  # max_lat\n",
    "            bbox[3]   # max_lon\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load CSV for per-tile analysis\n",
    "    csv_files = list(extract_dir.rglob('*_info.csv'))\n",
    "    if csv_files:\n",
    "        tiles_df = pd.read_csv(csv_files[0])\n",
    "    else:\n",
    "        tiles_df = None\n",
    "\n",
    "print(\"\\nTile Info:\")\n",
    "print(f\"  Center: {tile_info['center']}\")\n",
    "print(f\"  Zoom: {tile_info['zoom']}\")\n",
    "print(f\"  BBox: {tile_info['bbox']}\")\n",
    "\n",
    "if tiles_df is not None:\n",
    "    print(f\"\\n✓ Loaded {len(tiles_df)} tiles for per-tile analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML-detected network (paths)\n",
    "network_shp = list(extract_dir.rglob('*Network*.shp'))[0]\n",
    "ml_network = gpd.read_file(network_shp)\n",
    "\n",
    "# Ensure CRS is WGS84\n",
    "if ml_network.crs is None:\n",
    "    ml_network = ml_network.set_crs('EPSG:4326')\n",
    "elif ml_network.crs.to_string() != 'EPSG:4326':\n",
    "    ml_network = ml_network.to_crs('EPSG:4326')\n",
    "\n",
    "print(f\"\\n✓ Loaded ML Network: {len(ml_network)} features\")\n",
    "print(f\"  Geometry types: {ml_network.geometry.type.value_counts().to_dict()}\")\n",
    "print(f\"  CRS: {ml_network.crs}\")\n",
    "print(f\"  Bounds: {ml_network.total_bounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML-detected polygons\n",
    "polygon_shp = list(extract_dir.rglob('*Polygon*.shp'))[0]\n",
    "ml_polygons = gpd.read_file(polygon_shp)\n",
    "\n",
    "if ml_polygons.crs is None:\n",
    "    ml_polygons = ml_polygons.set_crs('EPSG:4326')\n",
    "elif ml_polygons.crs.to_string() != 'EPSG:4326':\n",
    "    ml_polygons = ml_polygons.to_crs('EPSG:4326')\n",
    "\n",
    "print(f\"\\n✓ Loaded ML Polygons: {len(ml_polygons)} features\")\n",
    "print(f\"  Geometry types: {ml_polygons.geometry.type.value_counts().to_dict()}\")\n",
    "print(f\"  CRS: {ml_polygons.crs}\")\n",
    "print(f\"  Bounds: {ml_polygons.total_bounds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa235b8",
   "metadata": {},
   "source": [
    "## 4. Fetch OSM Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_osm_paths(bbox):\n",
    "    \"\"\"\n",
    "    Fetch OSM pedestrian paths using Overpass API\n",
    "    bbox: [min_lat, min_lon, max_lat, max_lon]\n",
    "    \"\"\"\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json][timeout:60];\n",
    "    (\n",
    "      way[\"highway\"=\"footway\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"highway\"=\"pedestrian\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"highway\"=\"path\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"highway\"=\"steps\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"footway\"=\"sidewalk\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"sidewalk\"=\"both\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"sidewalk\"=\"left\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"sidewalk\"=\"right\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"sidewalk\"=\"yes\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "    );\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    \n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    print(f\"Querying Overpass API for bbox: {bbox}\")\n",
    "    print(\"This may take 10-30 seconds...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(overpass_url, data={'data': overpass_query}, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"✓ Received {len(data.get('elements', []))} OSM elements\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error fetching OSM data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch OSM data\n",
    "osm_data = fetch_osm_paths(tile_info['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def osm_to_geodataframe(osm_data, bbox=None, segment_length=30):\n",
    "    \"\"\"\n",
    "    Convert Overpass API JSON to GeoDataFrame\n",
    "    Clips paths to bbox and splits long ways into segments\n",
    "    \n",
    "    Args:\n",
    "        osm_data: Raw Overpass API response\n",
    "        bbox: Bounding box [min_lat, min_lon, max_lat, max_lon] to clip to\n",
    "        segment_length: Maximum segment length in meters (default 30m)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Create a polygon from bbox for clipping\n",
    "    clip_box = None\n",
    "    if bbox:\n",
    "        clip_box = box(bbox[1], bbox[0], bbox[3], bbox[2])  # (minlon, minlat, maxlon, maxlat)\n",
    "    \n",
    "    for element in osm_data.get('elements', []):\n",
    "        if element['type'] == 'way' and 'geometry' in element:\n",
    "            coords = [(node['lon'], node['lat']) for node in element['geometry']]\n",
    "            \n",
    "            if len(coords) >= 2:\n",
    "                tags = element.get('tags', {})\n",
    "                osm_id = element['id']\n",
    "\n",
    "                # Check if this is a cyclic area (closed polygon)\n",
    "                is_closed = coords[0] == coords[-1]\n",
    "                is_area = tags.get('area') == 'yes' and is_closed\n",
    "                # or ( tags.get('highway') == 'pedestrian' and len(coords) > 4 and is_closed )\n",
    "                # but doesn't seem to actually fit OSM standards\n",
    "\n",
    "                if is_area:\n",
    "                    # Skip polygonal pedestrian areas - only validate linear paths\n",
    "                    continue\n",
    "                \n",
    "                # Create LineString for this way\n",
    "                way_line = LineString(coords)\n",
    "                \n",
    "                # Clip to bbox if provided\n",
    "                if clip_box:\n",
    "                    try:\n",
    "                        clipped = way_line.intersection(clip_box)\n",
    "                        # Handle different geometry types from intersection\n",
    "                        if clipped.is_empty:\n",
    "                            continue\n",
    "                        if clipped.geom_type == 'LineString':\n",
    "                            way_line = clipped\n",
    "                        elif clipped.geom_type == 'MultiLineString':\n",
    "                            # For MultiLineString, process each part separately\n",
    "                            for geom in clipped.geoms:\n",
    "                                if len(geom.coords) >= 2:\n",
    "                                    segments = split_linestring_by_length(geom, segment_length)\n",
    "                                    for seg_idx, segment in enumerate(segments):\n",
    "                                        features.append({\n",
    "                                            'geometry': segment,\n",
    "                                            'osm_id': osm_id,\n",
    "                                            'segment_index': seg_idx,\n",
    "                                            'highway': tags.get('highway', ''),\n",
    "                                            'footway': tags.get('footway', ''),\n",
    "                                            'sidewalk': tags.get('sidewalk', ''),\n",
    "                                            'name': tags.get('name', '')\n",
    "                                        })\n",
    "                            continue\n",
    "                        else:\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Failed to clip way {osm_id}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Split the way into segments\n",
    "                segments = split_linestring_by_length(way_line, segment_length)\n",
    "                \n",
    "                # Add each segment as a separate feature\n",
    "                for seg_idx, segment in enumerate(segments):\n",
    "                    if len(segment.coords) >= 2:\n",
    "                        features.append({\n",
    "                            'geometry': segment,\n",
    "                            'osm_id': osm_id,\n",
    "                            'segment_index': seg_idx,\n",
    "                            'highway': tags.get('highway', ''),\n",
    "                            'footway': tags.get('footway', ''),\n",
    "                            'sidewalk': tags.get('sidewalk', ''),\n",
    "                            'name': tags.get('name', '')\n",
    "                        })\n",
    "    \n",
    "    if not features:\n",
    "        print(\"⚠️ No valid OSM paths found!\")\n",
    "        return gpd.GeoDataFrame()\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(features, crs='EPSG:4326')\n",
    "    return gdf\n",
    "\n",
    "def split_linestring_by_length(line, max_segment_length):\n",
    "    \"\"\"\n",
    "    Split a LineString into segments of approximately max_segment_length meters\n",
    "    \n",
    "    Args:\n",
    "        line: shapely LineString in WGS84\n",
    "        max_segment_length: Maximum length per segment in meters\n",
    "        \n",
    "    Returns:\n",
    "        List of LineString segments\n",
    "    \"\"\"\n",
    "    # Project to UTM for accurate length measurements\n",
    "    line_utm = gpd.GeoSeries([line], crs='EPSG:4326').to_crs('EPSG:32619')[0]\n",
    "    \n",
    "    total_length = line_utm.length\n",
    "    \n",
    "    # If line is shorter than max segment, return as-is\n",
    "    if total_length <= max_segment_length:\n",
    "        return [line]\n",
    "    \n",
    "    # Calculate number of segments needed\n",
    "    num_segments = math.ceil(total_length / max_segment_length)\n",
    "    segment_length = total_length / num_segments\n",
    "    \n",
    "    segments = []\n",
    "    \n",
    "    # Extract points along the line at regular intervals\n",
    "    for i in range(num_segments):\n",
    "        start_dist = i * segment_length\n",
    "        end_dist = (i + 1) * segment_length\n",
    "        \n",
    "        # Extract segment endpoints using interpolate\n",
    "        start_point = line_utm.interpolate(start_dist)\n",
    "        end_dist_actual = min(end_dist, total_length)\n",
    "        end_point = line_utm.interpolate(end_dist_actual)\n",
    "        \n",
    "        # Build coordinate list for this segment\n",
    "        coords = [start_point.coords[0]]\n",
    "        \n",
    "        # Add all original coordinates that fall within this segment\n",
    "        for coord in line_utm.coords:\n",
    "            point = Point(coord)\n",
    "            dist_along = line_utm.project(point)\n",
    "            if start_dist < dist_along < end_dist_actual:\n",
    "                coords.append(coord)\n",
    "        \n",
    "        # Add end point\n",
    "        coords.append(end_point.coords[0])\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_coords = []\n",
    "        for coord in coords:\n",
    "            if not unique_coords or coord != unique_coords[-1]:\n",
    "                unique_coords.append(coord)\n",
    "        \n",
    "        # Create segment line\n",
    "        if len(unique_coords) >= 2:\n",
    "            segment_utm = LineString(unique_coords)\n",
    "            # Convert back to WGS84\n",
    "            segment_wgs84 = gpd.GeoSeries([segment_utm], crs='EPSG:32619').to_crs('EPSG:4326')[0]\n",
    "            segments.append(segment_wgs84)\n",
    "    \n",
    "    return segments if segments else [line]\n",
    "\n",
    "# Convert to GeoDataFrame with segmentation and clipping\n",
    "if osm_data:\n",
    "    osm_paths = osm_to_geodataframe(osm_data, bbox=tile_info['bbox'], segment_length=1)\n",
    "    print(f\"\\n✓ Created OSM GeoDataFrame: {len(osm_paths)} features\")\n",
    "    print(f\"  Original ways: {osm_paths['osm_id'].nunique()}\")\n",
    "    print(f\"  Highway types: {osm_paths['highway'].value_counts().to_dict()}\")\n",
    "    print(f\"  Bounds: {osm_paths.total_bounds}\")\n",
    "else:\n",
    "    osm_paths = gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2fe0e",
   "metadata": {},
   "source": [
    "## 4.5. Area Composition Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_osm_area_features(bbox):\n",
    "    \"\"\"\n",
    "    Fetch OSM features for area composition analysis\n",
    "    bbox: [min_lat, min_lon, max_lat, max_lon]\n",
    "    \"\"\"\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json][timeout:60];\n",
    "    (\n",
    "      way[\"building\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"building\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      \n",
    "      way[\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|service|unclassified\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      \n",
    "      way[\"highway\"=\"pedestrian\"][\"area\"=\"yes\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"highway\"=\"pedestrian\"][\"area\"=\"yes\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      \n",
    "      way[\"landuse\"~\"grass|meadow|recreation_ground\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"leisure\"~\"park|garden|playground\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"natural\"~\"grassland|scrub\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"landuse\"~\"grass|meadow|recreation_ground\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"leisure\"~\"park|garden|playground\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      \n",
    "      way[\"natural\"=\"water\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"waterway\"~\"river|stream\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"natural\"=\"water\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      \n",
    "      way[\"railway\"~\"rail|tram\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      \n",
    "      way[\"amenity\"=\"parking\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"amenity\"=\"parking\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "    );\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    \n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(overpass_url, data={'data': overpass_query}, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching OSM area features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ef60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_road_width(tags):\n",
    "    \"\"\"\n",
    "    Estimate road width in meters based on OSM tags\n",
    "    \"\"\"\n",
    "    # Check for explicit width tag\n",
    "    if 'width' in tags:\n",
    "        try:\n",
    "            return float(tags['width'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    highway_type = tags.get('highway', '')\n",
    "    lanes = tags.get('lanes', None)\n",
    "    \n",
    "    # Lane width assumptions\n",
    "    lane_widths = {\n",
    "        'motorway': 3.5,\n",
    "        'trunk': 3.5,\n",
    "        'primary': 3.25,\n",
    "        'secondary': 3.25,\n",
    "        'tertiary': 3.0,\n",
    "        'residential': 3.0,\n",
    "        'unclassified': 3.0,\n",
    "        'service': 2.5\n",
    "    }\n",
    "    \n",
    "    # Default lane counts if not specified\n",
    "    default_lanes = {\n",
    "        'motorway': 2,\n",
    "        'trunk': 2,\n",
    "        'primary': 2,\n",
    "        'secondary': 2,\n",
    "        'tertiary': 1,\n",
    "        'residential': 1,\n",
    "        'unclassified': 1,\n",
    "        'service': 1\n",
    "    }\n",
    "    \n",
    "    lane_width = lane_widths.get(highway_type, 3.0)\n",
    "    \n",
    "    if lanes:\n",
    "        try:\n",
    "            num_lanes = int(lanes)\n",
    "            return lane_width * num_lanes\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return lane_width * default_lanes.get(highway_type, 1)\n",
    "\n",
    "\n",
    "def estimate_railway_width(tags):\n",
    "    \"\"\"\n",
    "    Estimate railway width in meters based on OSM tags\n",
    "    \"\"\"\n",
    "    tracks = tags.get('tracks', '1')\n",
    "    try:\n",
    "        num_tracks = int(tracks)\n",
    "        return 1.5 * num_tracks  # Standard gauge track width\n",
    "    except:\n",
    "        return 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tile_area_composition(tile_bbox, osm_features_data):\n",
    "    \"\"\"\n",
    "    Compute area composition for a tile\n",
    "    \n",
    "    Args:\n",
    "        tile_bbox: shapely box for the tile\n",
    "        osm_features_data: OSM Overpass API response with area features\n",
    "    \n",
    "    Returns:\n",
    "        dict with area percentages for each category\n",
    "    \"\"\"\n",
    "    if not osm_features_data or 'elements' not in osm_features_data:\n",
    "        return None\n",
    "    \n",
    "    # Calculate total tile area (in square meters)\n",
    "    tile_gdf = gpd.GeoDataFrame([{'geometry': tile_bbox}], crs='EPSG:4326')\n",
    "    tile_utm = tile_gdf.to_crs('EPSG:32619')\n",
    "    total_area = tile_utm.geometry.area.sum()\n",
    "    \n",
    "    # Initialize category geometries\n",
    "    categories = {\n",
    "        'buildings': [],\n",
    "        'roads': [],\n",
    "        'pedestrian_areas': [],\n",
    "        'green_spaces': [],\n",
    "        'water': [],\n",
    "        'railways': [],\n",
    "        'parking': []\n",
    "    }\n",
    "    \n",
    "    for element in osm_features_data['elements']:\n",
    "        tags = element.get('tags', {})\n",
    "        geom = None\n",
    "        \n",
    "        # Parse geometry based on element type\n",
    "        if element['type'] == 'way' and 'geometry' in element:\n",
    "            coords = [(node['lon'], node['lat']) for node in element['geometry']]\n",
    "            \n",
    "            # Buildings\n",
    "            if 'building' in tags:\n",
    "                if len(coords) >= 3:\n",
    "                    geom = Polygon(coords) if coords[0] == coords[-1] else Polygon(coords + [coords[0]])\n",
    "                    categories['buildings'].append(geom)\n",
    "            \n",
    "            # Roads (convert to buffered polygon)\n",
    "            elif 'highway' in tags and tags['highway'] in ['motorway', 'trunk', 'primary', 'secondary', \n",
    "                                                            'tertiary', 'residential', 'service', 'unclassified']:\n",
    "                if len(coords) >= 2:\n",
    "                    line = LineString(coords)\n",
    "                    # Clip to tile\n",
    "                    line = line.intersection(tile_bbox)\n",
    "                    if not line.is_empty:\n",
    "                        width = estimate_road_width(tags)\n",
    "                        # Buffer in UTM for accurate width\n",
    "                        line_gdf = gpd.GeoDataFrame([{'geometry': line}], crs='EPSG:4326')\n",
    "                        line_utm = line_gdf.to_crs('EPSG:32619').geometry[0]\n",
    "                        buffered = line_utm.buffer(width / 2)\n",
    "                        # Convert back to WGS84\n",
    "                        buffered_gdf = gpd.GeoDataFrame([{'geometry': buffered}], crs='EPSG:32619')\n",
    "                        geom = buffered_gdf.to_crs('EPSG:4326').geometry[0]\n",
    "                        categories['roads'].append(geom)\n",
    "            \n",
    "            # Pedestrian areas\n",
    "            elif tags.get('highway') == 'pedestrian' and tags.get('area') == 'yes':\n",
    "                if len(coords) >= 3:\n",
    "                    geom = Polygon(coords) if coords[0] == coords[-1] else Polygon(coords + [coords[0]])\n",
    "                    categories['pedestrian_areas'].append(geom)\n",
    "            \n",
    "            # Green spaces\n",
    "            elif (tags.get('landuse') in ['grass', 'meadow', 'recreation_ground'] or\n",
    "                  tags.get('leisure') in ['park', 'garden', 'playground'] or\n",
    "                  tags.get('natural') in ['grassland', 'scrub']):\n",
    "                if len(coords) >= 3:\n",
    "                    geom = Polygon(coords) if coords[0] == coords[-1] else Polygon(coords + [coords[0]])\n",
    "                    categories['green_spaces'].append(geom)\n",
    "            \n",
    "            # Water\n",
    "            elif tags.get('natural') == 'water':\n",
    "                if len(coords) >= 3:\n",
    "                    geom = Polygon(coords) if coords[0] == coords[-1] else Polygon(coords + [coords[0]])\n",
    "                    categories['water'].append(geom)\n",
    "            elif tags.get('waterway') in ['river', 'stream']:\n",
    "                if len(coords) >= 2:\n",
    "                    line = LineString(coords)\n",
    "                    line = line.intersection(tile_bbox)\n",
    "                    if not line.is_empty:\n",
    "                        # Buffer waterways by 5m default\n",
    "                        line_gdf = gpd.GeoDataFrame([{'geometry': line}], crs='EPSG:4326')\n",
    "                        line_utm = line_gdf.to_crs('EPSG:32619').geometry[0]\n",
    "                        buffered = line_utm.buffer(5)\n",
    "                        buffered_gdf = gpd.GeoDataFrame([{'geometry': buffered}], crs='EPSG:32619')\n",
    "                        geom = buffered_gdf.to_crs('EPSG:4326').geometry[0]\n",
    "                        categories['water'].append(geom)\n",
    "            \n",
    "            # Railways\n",
    "            elif tags.get('railway') in ['rail', 'tram']:\n",
    "                if len(coords) >= 2:\n",
    "                    line = LineString(coords)\n",
    "                    line = line.intersection(tile_bbox)\n",
    "                    if not line.is_empty:\n",
    "                        width = estimate_railway_width(tags)\n",
    "                        line_gdf = gpd.GeoDataFrame([{'geometry': line}], crs='EPSG:4326')\n",
    "                        line_utm = line_gdf.to_crs('EPSG:32619').geometry[0]\n",
    "                        buffered = line_utm.buffer(width / 2)\n",
    "                        buffered_gdf = gpd.GeoDataFrame([{'geometry': buffered}], crs='EPSG:32619')\n",
    "                        geom = buffered_gdf.to_crs('EPSG:4326').geometry[0]\n",
    "                        categories['railways'].append(geom)\n",
    "            \n",
    "            # Parking\n",
    "            elif tags.get('amenity') == 'parking':\n",
    "                if len(coords) >= 3:\n",
    "                    geom = Polygon(coords) if coords[0] == coords[-1] else Polygon(coords + [coords[0]])\n",
    "                    categories['parking'].append(geom)\n",
    "    \n",
    "    # Calculate areas\n",
    "    result = {}\n",
    "    covered_area = 0\n",
    "    \n",
    "    for category, geoms in categories.items():\n",
    "        if geoms:\n",
    "            # Clip to tile and calculate area\n",
    "            clipped_geoms = []\n",
    "            for geom in geoms:\n",
    "                try:\n",
    "                    clipped = geom.intersection(tile_bbox)\n",
    "                    if not clipped.is_empty:\n",
    "                        clipped_geoms.append(clipped)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if clipped_geoms:\n",
    "                # Union and convert to UTM for accurate area\n",
    "                category_gdf = gpd.GeoDataFrame([{'geometry': g} for g in clipped_geoms], crs='EPSG:4326')\n",
    "                category_utm = category_gdf.to_crs('EPSG:32619')\n",
    "                \n",
    "                # Union all geometries in category to avoid double-counting overlaps\n",
    "                try:\n",
    "                    unified = unary_union(category_utm.geometry)\n",
    "                    area = unified.area if hasattr(unified, 'area') else 0\n",
    "                except:\n",
    "                    area = category_utm.geometry.area.sum()\n",
    "                \n",
    "                covered_area += area\n",
    "                result[f'{category}_area_sqm'] = round(area, 2)\n",
    "                result[f'{category}_pct'] = round((area / total_area) * 100, 2) if total_area > 0 else 0\n",
    "            else:\n",
    "                result[f'{category}_area_sqm'] = 0\n",
    "                result[f'{category}_pct'] = 0\n",
    "        else:\n",
    "            result[f'{category}_area_sqm'] = 0\n",
    "            result[f'{category}_pct'] = 0\n",
    "    \n",
    "    # Calculate unmapped percentage\n",
    "    unmapped_area = max(0, total_area - covered_area)\n",
    "    result['unmapped_area_sqm'] = round(unmapped_area, 2)\n",
    "    result['unmapped_pct'] = round((unmapped_area / total_area) * 100, 2) if total_area > 0 else 0\n",
    "    result['total_area_sqm'] = round(total_area, 2)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810245e",
   "metadata": {},
   "source": [
    "## 5. Polygon-Based Confusion Matrix Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a60033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length_meters(gdf):\n",
    "    \"\"\"\n",
    "    Compute total length in meters using a projected CRS\n",
    "    \"\"\"\n",
    "    if len(gdf) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Project to UTM Zone 19N for accurate length calculation\n",
    "    gdf_projected = gdf.to_crs('EPSG:32619')\n",
    "    return gdf_projected.geometry.length.sum()\n",
    "\n",
    "# Calculate initial lengths\n",
    "ml_length = compute_length_meters(ml_network)\n",
    "osm_length = compute_length_meters(osm_paths)\n",
    "ml_polygon_area = ml_polygons.to_crs('EPSG:32619').geometry.area.sum()\n",
    "\n",
    "print(f\"ML Network Total Length: {ml_length:.2f} meters\")\n",
    "print(f\"OSM Paths Total Length: {osm_length:.2f} meters\")\n",
    "print(f\"ML Polygons Total Area: {ml_polygon_area:.2f} square meters\")\n",
    "print(f\"ML Polygons Count: {len(ml_polygons)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31015d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_polygon_based_confusion_matrix(ml_network_gdf, ml_polygons_gdf, osm_gdf, buffer_dist=3):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix using polygon-based validation\n",
    "    \n",
    "    Logic:\n",
    "    1. True Positive (TP): ML network segments within ML polygons that also contain OSM segments\n",
    "    2. False Negative (FN): OSM segments outside all ML polygons\n",
    "    3. False Positive (FP): ML network segments within ML polygons that contain no OSM segments\n",
    "\n",
    "    OSM segments are split by polygon boundaries so that parts inside and outside polygons\n",
    "    are evaluated separately. This handles cases where a path spans multiple polygons.\n",
    "    \n",
    "    Args:\n",
    "        ml_network_gdf: ML detected network paths GeoDataFrame\n",
    "        ml_polygons_gdf: ML detected polygons GeoDataFrame\n",
    "        osm_gdf: OSM ground truth paths GeoDataFrame\n",
    "        buffer_dist: Buffer distance in meters for segment matching within polygons\n",
    "    \n",
    "    Returns:\n",
    "        tp_gdf, fp_gdf, fn_gdf, tp_osm_gdf, metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nComputing polygon-based confusion matrix (buffer={buffer_dist}m)...\")\n",
    "    \n",
    "    if len(ml_network_gdf) == 0 or len(osm_gdf) == 0 or len(ml_polygons_gdf) == 0:\n",
    "        print(\"⚠️ One or more datasets are empty!\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Project all data to UTM for accurate spatial operations\n",
    "    print(\"  Projecting to UTM...\")\n",
    "    ml_network_utm = ml_network_gdf.to_crs('EPSG:32619')\n",
    "    ml_polygons_utm = ml_polygons_gdf.to_crs('EPSG:32619')\n",
    "    osm_utm = osm_gdf.to_crs('EPSG:32619')\n",
    "    \n",
    "    # Create unified polygon coverage (union of all ML polygons)\n",
    "    print(\"  Creating unified polygon coverage...\")\n",
    "    unified_polygon = unary_union(ml_polygons_utm.geometry)\n",
    "    \n",
    "    # Step 1: Split OSM segments by polygon boundaries\n",
    "    print(\"  Splitting OSM segments by polygon boundaries...\")\n",
    "    osm_segments_inside = []\n",
    "    osm_segments_outside = []\n",
    "    \n",
    "    for idx, osm_geom in enumerate(osm_utm.geometry):\n",
    "        try:\n",
    "            # Get the part inside polygons\n",
    "            inside_part = osm_geom.intersection(unified_polygon)\n",
    "            \n",
    "            # Get the part outside polygons\n",
    "            outside_part = osm_geom.difference(unified_polygon)\n",
    "            \n",
    "            # Add inside parts (can be LineString or MultiLineString)\n",
    "            if not inside_part.is_empty:\n",
    "                if inside_part.geom_type == 'LineString':\n",
    "                    if len(inside_part.coords) >= 2:\n",
    "                        osm_segments_inside.append({\n",
    "                            'geometry': inside_part,\n",
    "                            'original_idx': idx\n",
    "                        })\n",
    "                elif inside_part.geom_type == 'MultiLineString':\n",
    "                    for line in inside_part.geoms:\n",
    "                        if len(line.coords) >= 2:\n",
    "                            osm_segments_inside.append({\n",
    "                                'geometry': line,\n",
    "                                'original_idx': idx\n",
    "                            })\n",
    "            \n",
    "            # Add outside parts\n",
    "            if not outside_part.is_empty:\n",
    "                if outside_part.geom_type == 'LineString':\n",
    "                    if len(outside_part.coords) >= 2:\n",
    "                        osm_segments_outside.append({\n",
    "                            'geometry': outside_part,\n",
    "                            'original_idx': idx\n",
    "                        })\n",
    "                elif outside_part.geom_type == 'MultiLineString':\n",
    "                    for line in outside_part.geoms:\n",
    "                        if len(line.coords) >= 2:\n",
    "                            osm_segments_outside.append({\n",
    "                                'geometry': line,\n",
    "                                'original_idx': idx\n",
    "                            })\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Failed to split OSM segment {idx}: {e}\")\n",
    "            # If splitting fails, classify the whole segment based on centroid\n",
    "            if osm_geom.centroid.within(unified_polygon):\n",
    "                osm_segments_inside.append({\n",
    "                    'geometry': osm_geom,\n",
    "                    'original_idx': idx\n",
    "                })\n",
    "            else:\n",
    "                osm_segments_outside.append({\n",
    "                    'geometry': osm_geom,\n",
    "                    'original_idx': idx\n",
    "                })\n",
    "    \n",
    "    print(f\"    Split into {len(osm_segments_inside)} inside parts and {len(osm_segments_outside)} outside parts\")\n",
    "    \n",
    "    # Create GeoDataFrames for inside and outside OSM segments\n",
    "    if osm_segments_inside:\n",
    "        osm_inside_gdf = gpd.GeoDataFrame(\n",
    "            osm_segments_inside, \n",
    "            crs='EPSG:32619'\n",
    "        )\n",
    "    else:\n",
    "        osm_inside_gdf = gpd.GeoDataFrame(columns=['geometry', 'original_idx'], crs='EPSG:32619')\n",
    "    \n",
    "    if osm_segments_outside:\n",
    "        osm_outside_gdf = gpd.GeoDataFrame(\n",
    "            osm_segments_outside,\n",
    "            crs='EPSG:32619'\n",
    "        )\n",
    "        # Convert back to WGS84 for output\n",
    "        fn_gdf = osm_outside_gdf.to_crs('EPSG:4326')\n",
    "        fn_length = osm_outside_gdf.geometry.length.sum()\n",
    "    else:\n",
    "        fn_gdf = gpd.GeoDataFrame(columns=['geometry', 'original_idx'], crs='EPSG:4326')\n",
    "        fn_length = 0.0\n",
    "    \n",
    "    print(f\"    False Negative length (outside polygons): {fn_length:.2f}m\")\n",
    "    \n",
    "    # Step 2: Find ML network segments within polygons\n",
    "    print(\"  Filtering ML network segments within polygons...\")\n",
    "    ml_in_polygon_indices = []\n",
    "    for idx, ml_geom in enumerate(ml_network_utm.geometry):\n",
    "        if ml_geom.intersects(unified_polygon):\n",
    "            ml_in_polygon_indices.append(idx)\n",
    "    \n",
    "    ml_in_polygons = ml_network_gdf.iloc[ml_in_polygon_indices].copy()\n",
    "    ml_in_polygons_utm = ml_network_utm.iloc[ml_in_polygon_indices].copy()\n",
    "    \n",
    "    print(f\"    Found {len(ml_in_polygons)} ML segments within polygons\")\n",
    "    \n",
    "    # Step 3: Match ML segments with OSM segments inside polygons\n",
    "    print(\"  Matching ML segments with OSM segments (with buffer)...\")\n",
    "    \n",
    "    if len(osm_inside_gdf) == 0:\n",
    "        # No OSM segments inside polygons, all ML segments are false positives\n",
    "        tp_gdf = gpd.GeoDataFrame(columns=ml_network_gdf.columns, crs='EPSG:4326')\n",
    "        fp_gdf = ml_in_polygons.copy()\n",
    "        tp_osm_gdf = gpd.GeoDataFrame(columns=['geometry', 'original_idx'], crs='EPSG:4326')\n",
    "        tp_length = 0.0\n",
    "        fp_length = ml_in_polygons_utm.geometry.length.sum()\n",
    "        total_osm_in_polygons = 0.0\n",
    "    else:\n",
    "        # Create buffered OSM segments for matching\n",
    "        osm_buffered = osm_inside_gdf.copy()\n",
    "        osm_buffered['geometry'] = osm_inside_gdf.geometry.buffer(buffer_dist)\n",
    "        \n",
    "        # Build spatial index\n",
    "        osm_sindex = osm_buffered.sindex\n",
    "        \n",
    "        tp_indices = []\n",
    "        fp_indices = []\n",
    "        tp_osm_indices = set()  # Track which OSM segments were matched\n",
    "        \n",
    "        for idx, ml_geom in enumerate(ml_in_polygons_utm.geometry):\n",
    "            # Find candidate OSM matches\n",
    "            possible_matches = list(osm_sindex.intersection(ml_geom.bounds))\n",
    "            \n",
    "            has_match = False\n",
    "            for osm_idx in possible_matches:\n",
    "                osm_geom_buffered = osm_buffered.iloc[osm_idx].geometry\n",
    "                \n",
    "                # Check if ML segment intersects buffered OSM segment\n",
    "                if ml_geom.intersects(osm_geom_buffered):\n",
    "                    has_match = True\n",
    "                    tp_osm_indices.add(osm_idx)  # Track matched OSM segment\n",
    "                    break\n",
    "            \n",
    "            if has_match:\n",
    "                tp_indices.append(ml_in_polygon_indices[idx])\n",
    "            else:\n",
    "                fp_indices.append(ml_in_polygon_indices[idx])\n",
    "        \n",
    "        # Create result GeoDataFrames\n",
    "        tp_gdf = ml_network_gdf.iloc[tp_indices].copy()\n",
    "        fp_gdf = ml_network_gdf.iloc[fp_indices].copy()\n",
    "        # Create OSM TP GeoDataFrame from matched segments\n",
    "        if tp_osm_indices:\n",
    "            tp_osm_gdf = osm_inside_gdf.iloc[list(tp_osm_indices)].copy().to_crs('EPSG:4326')\n",
    "        else:\n",
    "            tp_osm_gdf = gpd.GeoDataFrame(columns=['geometry', 'original_idx'], crs='EPSG:4326')\n",
    "        \n",
    "        # Calculate lengths\n",
    "        tp_length = ml_network_utm.iloc[[ml_in_polygon_indices.index(i) for i in tp_indices]].geometry.length.sum() if tp_indices else 0.0\n",
    "        fp_length = ml_network_utm.iloc[[ml_in_polygon_indices.index(i) for i in fp_indices]].geometry.length.sum() if fp_indices else 0.0\n",
    "        total_osm_in_polygons = osm_inside_gdf.geometry.length.sum()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_ml_in_polygons = tp_length + fp_length\n",
    "    \n",
    "    precision = tp_length / total_ml_in_polygons if total_ml_in_polygons > 0 else 0\n",
    "    recall = tp_length / (tp_length + fn_length) if (tp_length + fn_length) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    iou = tp_length / (tp_length + fp_length + fn_length) if (tp_length + fp_length + fn_length) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'method': 'polygon_based_segmented',\n",
    "        'buffer_distance_m': buffer_dist,\n",
    "        'polygon_count': len(ml_polygons_gdf),\n",
    "        'polygon_area_sqm': round(ml_polygon_area, 2),\n",
    "        'tp_length_m': round(tp_length, 2),\n",
    "        'fp_length_m': round(fp_length, 2),\n",
    "        'fn_length_m': round(fn_length, 2),\n",
    "        'tp_count': len(tp_gdf),\n",
    "        'fp_count': len(fp_gdf),\n",
    "        'fn_count': len(fn_gdf),\n",
    "        'ml_total_length_m': round(compute_length_meters(ml_network_gdf), 2),\n",
    "        'ml_in_polygon_length_m': round(total_ml_in_polygons, 2),\n",
    "        'osm_total_length_m': round(compute_length_meters(osm_gdf), 2),\n",
    "        'osm_in_polygon_length_m': round(total_osm_in_polygons, 2),\n",
    "        'osm_outside_polygon_length_m': round(fn_length, 2),\n",
    "        'osm_inside_parts_count': len(osm_segments_inside),\n",
    "        'osm_outside_parts_count': len(osm_segments_outside),\n",
    "        'precision': round(precision, 4),\n",
    "        'recall': round(recall, 4),\n",
    "        'f1_score': round(f1_score, 4),\n",
    "        'iou': round(iou, 4)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n✓ Polygon-based analysis complete (with segmentation):\")\n",
    "    print(f\"  ML segments in polygons: {len(ml_in_polygons)} ({total_ml_in_polygons:.2f}m)\")\n",
    "    print(f\"  OSM parts in polygons: {len(osm_segments_inside)} ({total_osm_in_polygons:.2f}m)\")\n",
    "    print(f\"  OSM parts outside polygons: {len(osm_segments_outside)} ({fn_length:.2f}m)\")\n",
    "    print(f\"\\n  TP: {metrics['tp_count']} features, {metrics['tp_length_m']:.2f}m\")\n",
    "    print(f\"  FP: {metrics['fp_count']} features, {metrics['fp_length_m']:.2f}m\")\n",
    "    print(f\"  FN: {metrics['fn_count']} parts, {metrics['fn_length_m']:.2f}m\")\n",
    "\n",
    "    return tp_gdf, fp_gdf, fn_gdf, tp_osm_gdf, metrics\n",
    "\n",
    "# Compute polygon-based confusion matrix\n",
    "tp_global, fp_global, fn_global, tp_osm_global, metrics_global = compute_polygon_based_confusion_matrix(\n",
    "    ml_network, ml_polygons, osm_paths, BUFFER_DISTANCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974454e",
   "metadata": {},
   "source": [
    "## 6. Compute Per-Tile Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf75009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_tile_metrics_polygon_based(ml_network_gdf, ml_polygons_gdf, osm_gdf, tiles_df, buffer_dist=3, include_area_composition=True):\n",
    "    \"\"\"\n",
    "    Compute polygon-based confusion matrix for each tile with optional area composition analysis\n",
    "    \n",
    "    Args:\n",
    "        include_area_composition: If True, fetch OSM features and compute area composition for each tile\n",
    "    \"\"\"\n",
    "    if tiles_df is None or len(tiles_df) == 0:\n",
    "        print(\"⚠️ No tile information available for per-tile analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nComputing per-tile polygon-based metrics for {len(tiles_df)} tiles...\")\n",
    "    if include_area_composition:\n",
    "        print(\"  (Including area composition analysis - this may take longer)\")\n",
    "    \n",
    "    # Fetch OSM area features once for the entire study area\n",
    "    osm_area_data = None\n",
    "    if include_area_composition:\n",
    "        print(\"  Fetching OSM area features for entire study area...\")\n",
    "        study_area_bbox = [\n",
    "            tiles_df['bottomright_y'].min(),  # min_lat\n",
    "            tiles_df['topleft_x'].min(),      # min_lon\n",
    "            tiles_df['topleft_y'].max(),      # max_lat\n",
    "            tiles_df['bottomright_x'].max()   # max_lon\n",
    "        ]\n",
    "        osm_area_data = fetch_osm_area_features(study_area_bbox)\n",
    "        if osm_area_data:\n",
    "            print(f\"  ✓ Fetched {len(osm_area_data.get('elements', []))} OSM area features\")\n",
    "        else:\n",
    "            print(\"  ⚠️ Failed to fetch OSM area features\")\n",
    "    \n",
    "    tile_metrics = []\n",
    "    \n",
    "    for idx, tile in tiles_df.iterrows():\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processing tile {idx+1}/{len(tiles_df)}...\")\n",
    "        \n",
    "        # Create bounding box for this tile\n",
    "        tile_bbox = box(\n",
    "            tile['topleft_x'],      # min_lon\n",
    "            tile['bottomright_y'],  # min_lat\n",
    "            tile['bottomright_x'],  # max_lon\n",
    "            tile['topleft_y']       # max_lat\n",
    "        )\n",
    "        \n",
    "        # Calculate tile center\n",
    "        tile_center_lat = (tile['topleft_y'] + tile['bottomright_y']) / 2\n",
    "        tile_center_lon = (tile['topleft_x'] + tile['bottomright_x']) / 2\n",
    "        \n",
    "        # Compute area composition if OSM data was fetched\n",
    "        area_composition = None\n",
    "        if include_area_composition and osm_area_data:\n",
    "            area_composition = compute_tile_area_composition(tile_bbox, osm_area_data)\n",
    "        \n",
    "        # Filter features within this tile\n",
    "        ml_network_tile = ml_network_gdf[ml_network_gdf.geometry.intersects(tile_bbox)].copy()\n",
    "        ml_polygons_tile = ml_polygons_gdf[ml_polygons_gdf.geometry.intersects(tile_bbox)].copy()\n",
    "        osm_tile = osm_gdf[osm_gdf.geometry.intersects(tile_bbox)].copy()\n",
    "        \n",
    "        if len(ml_network_tile) == 0 and len(osm_tile) == 0:\n",
    "            # Empty tile\n",
    "            tile_metric = {\n",
    "                'tile_id': tile.get('idd', f\"tile_{idx}\"),\n",
    "                'xtile': int(tile.get('xtile', -1)),\n",
    "                'ytile': int(tile.get('ytile', -1)),\n",
    "                'lat': tile_center_lat,\n",
    "                'lon': tile_center_lon,\n",
    "                'ml_network_count': 0,\n",
    "                'ml_polygon_count': 0,\n",
    "                'osm_count': 0,\n",
    "                'tp_length_m': 0,\n",
    "                'fp_length_m': 0,\n",
    "                'fn_length_m': 0,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'f1_score': 0,\n",
    "                'iou': 0\n",
    "            }\n",
    "            # Add area composition if available\n",
    "            if area_composition:\n",
    "                tile_metric.update(area_composition)\n",
    "            tile_metrics.append(tile_metric)\n",
    "            continue\n",
    "        \n",
    "        # Compute confusion matrix for this tile\n",
    "        if len(ml_network_tile) > 0 and len(osm_tile) > 0 and len(ml_polygons_tile) > 0:\n",
    "            _, _, _, _, tile_metric = compute_polygon_based_confusion_matrix(\n",
    "                ml_network_tile, ml_polygons_tile, osm_tile, buffer_dist\n",
    "            )\n",
    "            if tile_metric:\n",
    "                tile_metric['tile_id'] = tile.get('idd', f\"tile_{idx}\")\n",
    "                tile_metric['xtile'] = int(tile.get('xtile', -1))\n",
    "                tile_metric['ytile'] = int(tile.get('ytile', -1))\n",
    "                tile_metric['lat'] = tile_center_lat\n",
    "                tile_metric['lon'] = tile_center_lon\n",
    "                tile_metric['ml_network_count'] = len(ml_network_tile)\n",
    "                tile_metric['ml_polygon_count'] = len(ml_polygons_tile)\n",
    "                tile_metric['osm_count'] = len(osm_tile)\n",
    "                # Add area composition if available\n",
    "                if area_composition:\n",
    "                    tile_metric.update(area_composition)\n",
    "                tile_metrics.append(tile_metric)\n",
    "        else:\n",
    "            # Handle partial data cases\n",
    "            ml_length = compute_length_meters(ml_network_tile) if len(ml_network_tile) > 0 else 0\n",
    "            osm_length = compute_length_meters(osm_tile) if len(osm_tile) > 0 else 0\n",
    "            \n",
    "            tile_metric = {\n",
    "                'tile_id': tile.get('idd', f\"tile_{idx}\"),\n",
    "                'xtile': int(tile.get('xtile', -1)),\n",
    "                'ytile': int(tile.get('ytile', -1)),\n",
    "                'lat': tile_center_lat,\n",
    "                'lon': tile_center_lon,\n",
    "                'ml_network_count': len(ml_network_tile),\n",
    "                'ml_polygon_count': len(ml_polygons_tile),\n",
    "                'osm_count': len(osm_tile),\n",
    "                'tp_length_m': 0,\n",
    "                'fp_length_m': ml_length if len(ml_polygons_tile) > 0 else 0,\n",
    "                'fn_length_m': osm_length,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'f1_score': 0,\n",
    "                'iou': 0\n",
    "            }\n",
    "            # Add area composition if available\n",
    "            if area_composition:\n",
    "                tile_metric.update(area_composition)\n",
    "            tile_metrics.append(tile_metric)\n",
    "    \n",
    "    print(f\"\\n✓ Computed metrics for {len(tile_metrics)} tiles\")\n",
    "    return tile_metrics\n",
    "\n",
    "# Compute per-tile metrics\n",
    "per_tile_metrics = compute_per_tile_metrics_polygon_based(\n",
    "    ml_network, ml_polygons, osm_paths, tiles_df, BUFFER_DISTANCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d1a77",
   "metadata": {},
   "source": [
    "## 6.5. Area Composition Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display area composition statistics if available\n",
    "if per_tile_metrics and 'buildings_pct' in per_tile_metrics[0]:\n",
    "    print(\"=\"*70)\n",
    "    print(\"AREA COMPOSITION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate average percentages across all tiles\n",
    "    categories = ['buildings', 'roads', 'pedestrian_areas', 'green_spaces', 'water', 'railways', 'parking', 'unmapped']\n",
    "    \n",
    "    print(\"\\nAverage Area Composition Across All Tiles:\")\n",
    "    for cat in categories:\n",
    "        pct_key = f'{cat}_pct'\n",
    "        if pct_key in per_tile_metrics[0]:\n",
    "            avg_pct = sum(t[pct_key] for t in per_tile_metrics) / len(per_tile_metrics)\n",
    "            print(f\"  {cat.replace('_', ' ').title():25s}: {avg_pct:6.2f}%\")\n",
    "    \n",
    "    # Correlation analysis: area composition vs performance\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CORRELATION: Area Composition vs Performance\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Filter tiles with valid F1 scores\n",
    "    valid_tiles = [t for t in per_tile_metrics if t['f1_score'] > 0]\n",
    "    \n",
    "    if len(valid_tiles) > 10:\n",
    "        import numpy as np\n",
    "        \n",
    "        print(f\"\\nAnalyzing {len(valid_tiles)} tiles with valid predictions...\")\n",
    "        \n",
    "        for cat in categories:\n",
    "            pct_key = f'{cat}_pct'\n",
    "            if pct_key in valid_tiles[0]:\n",
    "                pcts = [t[pct_key] for t in valid_tiles]\n",
    "                f1_scores = [t['f1_score'] for t in valid_tiles]\n",
    "                \n",
    "                # Calculate Pearson correlation\n",
    "                correlation = np.corrcoef(pcts, f1_scores)[0, 1]\n",
    "                \n",
    "                print(f\"  {cat.replace('_', ' ').title():25s} vs F1: {correlation:+.3f}\")\n",
    "        \n",
    "        # Find tiles with highest/lowest building coverage\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"Extreme Cases:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        sorted_by_buildings = sorted(valid_tiles, key=lambda x: x['buildings_pct'], reverse=True)\n",
    "        print(f\"\\nHighest Building Coverage:\")\n",
    "        for t in sorted_by_buildings[:3]:\n",
    "            print(f\"  Tile {t['tile_id']}: {t['buildings_pct']:.1f}% buildings, F1={t['f1_score']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nLowest Building Coverage:\")\n",
    "        for t in sorted_by_buildings[-3:]:\n",
    "            print(f\"  Tile {t['tile_id']}: {t['buildings_pct']:.1f}% buildings, F1={t['f1_score']:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Not enough tiles with valid predictions for correlation analysis\")\n",
    "else:\n",
    "    print(\"⚠️ Area composition data not available in per-tile metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14f99a",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa820b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory with polygon-specific name\n",
    "output_dir = Path('./polygon_analysis_output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Exporting polygon-based results...\")\n",
    "\n",
    "# 1. Export global confusion matrix\n",
    "if metrics_global:\n",
    "    with open(output_dir / 'confusion_matrix_global_polygon_based.json', 'w') as f:\n",
    "        json.dump(metrics_global, f, indent=2)\n",
    "    print(\"  ✓ confusion_matrix_global_polygon_based.json\")\n",
    "\n",
    "# 2. Export per-tile metrics\n",
    "if per_tile_metrics:\n",
    "    with open(output_dir / 'confusion_matrix_per_tile_polygon_based.json', 'w') as f:\n",
    "        json.dump(per_tile_metrics, f, indent=2)\n",
    "    print(\"  ✓ confusion_matrix_per_tile_polygon_based.json\")\n",
    "\n",
    "# 3. Export True Positives (ML network)\n",
    "if tp_global is not None and len(tp_global) > 0:\n",
    "    tp_global['category'] = 'true_positive'\n",
    "    tp_global.to_file(output_dir / 'true_positives_polygon_based.geojson', driver='GeoJSON')\n",
    "    print(f\"  ✓ true_positives_polygon_based.geojson ({len(tp_global)} features)\")\n",
    "\n",
    "# 3b. Export True Positives (OSM ground truth parts)\n",
    "if tp_osm_global is not None and len(tp_osm_global) > 0:\n",
    "    tp_osm_global['category'] = 'true_positive_osm'\n",
    "    tp_osm_global.to_file(output_dir / 'true_positives_osm_polygon_based.geojson', driver='GeoJSON')\n",
    "    print(f\"  ✓ true_positives_osm_polygon_based.geojson ({len(tp_osm_global)} OSM parts)\")\n",
    "\n",
    "# 4. Export False Positives\n",
    "if fp_global is not None and len(fp_global) > 0:\n",
    "    fp_global['category'] = 'false_positive'\n",
    "    fp_global.to_file(output_dir / 'false_positives_polygon_based.geojson', driver='GeoJSON')\n",
    "    print(f\"  ✓ false_positives_polygon_based.geojson ({len(fp_global)} features)\")\n",
    "\n",
    "# 5. Export False Negatives\n",
    "if fn_global is not None and len(fn_global) > 0:\n",
    "    fn_global['category'] = 'false_negative'\n",
    "    fn_global.to_file(output_dir / 'false_negatives_polygon_based.geojson', driver='GeoJSON')\n",
    "    print(f\"  ✓ false_negatives_polygon_based.geojson ({len(fn_global)} features)\")\n",
    "\n",
    "# 6. Export OSM ground truth\n",
    "if len(osm_paths) > 0:\n",
    "    osm_paths.to_file(output_dir / 'osm_ground_truth.geojson', driver='GeoJSON')\n",
    "    print(f\"  ✓ osm_ground_truth.geojson ({len(osm_paths)} features)\")\n",
    "\n",
    "# 7. Export ML polygons for reference\n",
    "if len(ml_polygons) > 0:\n",
    "    ml_polygons.to_file(output_dir / 'ml_polygons.geojson', driver='GeoJSON')\n",
    "    print(f\"  ✓ ml_polygons.geojson ({len(ml_polygons)} features)\")\n",
    "\n",
    "print(f\"\\n✓ All files exported to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2dd3a",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics\n",
    "\n",
    "This analysis uses POLYGON-BASED validation WITH SEGMENTATION:\n",
    "\n",
    "1. OSM ground truth paths are SPLIT by polygon boundaries into:\n",
    "   - Parts INSIDE polygons (used for TP/FP evaluation)\n",
    "   - Parts OUTSIDE polygons (counted as False Negatives)\n",
    "2. True Positive (TP): ML network segments within ML polygons that \n",
    "   match OSM segment parts inside polygons (within buffer distance)\n",
    "3. False Negative (FN): OSM segment PARTS that fall outside all ML polygons\n",
    "   (missed detection areas)\n",
    "4. False Positive (FP): ML network segments within ML polygons that \n",
    "   have NO matching OSM segments (over-detection within valid areas)\n",
    "\n",
    "Key Improvements:\n",
    "- Handles OSM paths spanning multiple polygons correctly\n",
    "- Each OSM path is split by polygon boundaries for accurate evaluation\n",
    "- A path fully covered by multiple polygons is considered fully detected\n",
    "- Only the parts truly outside polygons count as false negatives\n",
    "\n",
    "Area Composition Analysis:\n",
    "- Fetches comprehensive OSM features per tile (buildings, roads, parks, etc.)\n",
    "- Estimates road/railway widths from lane/track information\n",
    "- Calculates percentage of tile area occupied by each feature category\n",
    "- Correlates area composition with model performance metrics\n",
    "- Helps explain performance variations (e.g., high building density limits \n",
    "  aerial imagery visibility, affecting ML predictions)\n",
    "\n",
    "Benefits:\n",
    "- Better handles intersection parsing issues\n",
    "- More accurate recall calculation\n",
    "- Polygons define \"search areas\" for validation\n",
    "- Separates area detection (polygons) from path extraction (network)\n",
    "- Identifies environmental factors affecting model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba690e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(\"=\"*70)\n",
    "print(\"POLYGON-BASED CONFUSION MATRIX ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if metrics_global:\n",
    "    print(\"\\nGLOBAL METRICS:\")\n",
    "    print(f\"  Method: {metrics_global['method']}\")\n",
    "    print(f\"  Buffer Distance: {metrics_global['buffer_distance_m']}m\")\n",
    "    print(f\"  Polygon Count: {metrics_global['polygon_count']}\")\n",
    "    print(f\"  Polygon Area: {metrics_global['polygon_area_sqm']:.2f} m²\")\n",
    "    print(f\"\\n  ML Network Total Length: {metrics_global['ml_total_length_m']}m\")\n",
    "    print(f\"  ML Network in Polygons: {metrics_global['ml_in_polygon_length_m']}m\")\n",
    "    print(f\"  OSM Network Total Length: {metrics_global['osm_total_length_m']}m\")\n",
    "    print(f\"  OSM Network in Polygons: {metrics_global['osm_in_polygon_length_m']}m\")\n",
    "    print(f\"  OSM Network Outside Polygons: {metrics_global['osm_outside_polygon_length_m']}m\")\n",
    "    print(f\"\\n  True Positives:  {metrics_global['tp_length_m']}m ({metrics_global['tp_count']} features)\")\n",
    "    print(f\"  False Positives: {metrics_global['fp_length_m']}m ({metrics_global['fp_count']} features)\")\n",
    "    print(f\"  False Negatives: {metrics_global['fn_length_m']}m ({metrics_global['fn_count']} features)\")\n",
    "    print(f\"\\n  Precision: {metrics_global['precision']*100:.2f}%\")\n",
    "    print(f\"  Recall:    {metrics_global['recall']*100:.2f}%\")\n",
    "    print(f\"  F1 Score:  {metrics_global['f1_score']*100:.2f}%\")\n",
    "    print(f\"  IoU:       {metrics_global['iou']*100:.2f}%\")\n",
    "\n",
    "if per_tile_metrics:\n",
    "    print(f\"\\nPER-TILE METRICS:\")\n",
    "    print(f\"  Total Tiles: {len(per_tile_metrics)}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_precision = sum(t['precision'] for t in per_tile_metrics) / len(per_tile_metrics)\n",
    "    avg_recall = sum(t['recall'] for t in per_tile_metrics) / len(per_tile_metrics)\n",
    "    avg_f1 = sum(t['f1_score'] for t in per_tile_metrics) / len(per_tile_metrics)\n",
    "    avg_iou = sum(t['iou'] for t in per_tile_metrics) / len(per_tile_metrics)\n",
    "    \n",
    "    print(f\"  Avg Precision: {avg_precision*100:.2f}%\")\n",
    "    print(f\"  Avg Recall:    {avg_recall*100:.2f}%\")\n",
    "    print(f\"  Avg F1 Score:  {avg_f1*100:.2f}%\")\n",
    "    print(f\"  Avg IoU:       {avg_iou*100:.2f}%\")\n",
    "    \n",
    "    # Find best and worst tiles\n",
    "    sorted_by_f1 = sorted(per_tile_metrics, key=lambda x: x['f1_score'], reverse=True)\n",
    "    if len(sorted_by_f1) > 0:\n",
    "        best_tile = sorted_by_f1[0]\n",
    "        worst_tile = sorted_by_f1[-1]\n",
    "        print(f\"\\n  Best Tile: {best_tile['tile_id']} (F1={best_tile['f1_score']*100:.2f}%)\")\n",
    "        print(f\"  Worst Tile: {worst_tile['tile_id']} (F1={worst_tile['f1_score']*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d131e1",
   "metadata": {},
   "source": [
    "## 9. Visualization Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive map with polygon-based results\n",
    "import folium\n",
    "from folium import GeoJson\n",
    "\n",
    "# Create base map\n",
    "center = tile_info['center']\n",
    "m = folium.Map(location=center, zoom_start=17)\n",
    "\n",
    "# Add ML polygons layer (as background context)\n",
    "if len(ml_polygons) > 0:\n",
    "    ml_poly_geojson = json.loads(ml_polygons.to_json())\n",
    "    GeoJson(ml_poly_geojson, name='ML Polygons (Search Areas)', \n",
    "            style_function=lambda x: {\n",
    "                'fillColor': 'lightblue',\n",
    "                'color': 'blue',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.2\n",
    "            }).add_to(m)\n",
    "\n",
    "# Add ML network layer\n",
    "if len(ml_network) > 0:\n",
    "    ml_geojson = json.loads(ml_network.to_json())\n",
    "    GeoJson(ml_geojson, name='ML Network', \n",
    "            style_function=lambda x: {'color': 'purple', 'weight': 2, 'opacity': 0.6}).add_to(m)\n",
    "\n",
    "# Add OSM ground truth layer\n",
    "if len(osm_paths) > 0:\n",
    "    osm_geojson = json.loads(osm_paths.to_json())\n",
    "    GeoJson(osm_geojson, name='OSM Ground Truth', \n",
    "            style_function=lambda x: {'color': 'gray', 'weight': 2, 'opacity': 0.5}).add_to(m)\n",
    "\n",
    "# Add confusion matrix layers\n",
    "if tp_global is not None and len(tp_global) > 0:\n",
    "    tp_geojson = json.loads(tp_global.to_json())\n",
    "    GeoJson(tp_geojson, name='True Positives (in polygons + matched)', \n",
    "            style_function=lambda x: {'color': 'green', 'weight': 3, 'opacity': 0.8}).add_to(m)\n",
    "\n",
    "if fp_global is not None and len(fp_global) > 0:\n",
    "    fp_geojson = json.loads(fp_global.to_json())\n",
    "    GeoJson(fp_geojson, name='False Positives (in polygons + no match)', \n",
    "            style_function=lambda x: {'color': 'orange', 'weight': 3, 'opacity': 0.8}).add_to(m)\n",
    "\n",
    "if fn_global is not None and len(fn_global) > 0:\n",
    "    fn_geojson = json.loads(fn_global.to_json())\n",
    "    GeoJson(fn_geojson, name='False Negatives (outside polygons)', \n",
    "            style_function=lambda x: {'color': 'red', 'weight': 3, 'opacity': 0.8}).add_to(m)\n",
    "\n",
    "# Add bbox boundary\n",
    "bbox_coords = [\n",
    "    [tile_info['bbox'][0], tile_info['bbox'][1]],\n",
    "    [tile_info['bbox'][0], tile_info['bbox'][3]],\n",
    "    [tile_info['bbox'][2], tile_info['bbox'][3]],\n",
    "    [tile_info['bbox'][2], tile_info['bbox'][1]],\n",
    "    [tile_info['bbox'][0], tile_info['bbox'][1]]\n",
    "]\n",
    "folium.PolyLine(bbox_coords, color='black', weight=2, opacity=0.5, \n",
    "                popup='Study Area BBox').add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59aea6",
   "metadata": {},
   "source": [
    "## 10. Interactive Tile-Based Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198698b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive tile map with performance metrics and area composition\n",
    "if per_tile_metrics and tiles_df is not None:\n",
    "    import folium\n",
    "    from folium import GeoJson, Popup\n",
    "    import branca.colormap as cm\n",
    "    \n",
    "    # Create base map centered on study area\n",
    "    center = tile_info['center']\n",
    "    tile_map = folium.Map(location=center, zoom_start=15)\n",
    "    \n",
    "    # Create color scale based on F1 scores\n",
    "    f1_scores = [t['f1_score'] for t in per_tile_metrics if t['f1_score'] > 0]\n",
    "    \n",
    "    if f1_scores:\n",
    "        min_f1 = min(f1_scores)\n",
    "        max_f1 = max(f1_scores)\n",
    "        \n",
    "        # Create colormap from red (low) to green (high)\n",
    "        colormap = cm.LinearColormap(\n",
    "            colors=['#d73027', '#fee08b', '#1a9850'],\n",
    "            vmin=min_f1,\n",
    "            vmax=max_f1,\n",
    "            caption='F1 Score'\n",
    "        )\n",
    "        \n",
    "        # Add colormap to map\n",
    "        tile_map.add_child(colormap)\n",
    "        \n",
    "        # Add tiles as rectangles with color based on F1 score\n",
    "        for tile_metric in per_tile_metrics:\n",
    "            # Get tile from tiles_df\n",
    "            tile_row = tiles_df[tiles_df.get('idd', tiles_df.index) == tile_metric['tile_id']]\n",
    "            if len(tile_row) == 0:\n",
    "                # Try finding by index\n",
    "                try:\n",
    "                    tile_idx = int(tile_metric['tile_id'].replace('tile_', ''))\n",
    "                    tile_row = tiles_df.iloc[[tile_idx]]\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if len(tile_row) > 0:\n",
    "                tile = tile_row.iloc[0]\n",
    "                \n",
    "                # Create tile boundary\n",
    "                tile_coords = [\n",
    "                    [tile['topleft_y'], tile['topleft_x']],\n",
    "                    [tile['topleft_y'], tile['bottomright_x']],\n",
    "                    [tile['bottomright_y'], tile['bottomright_x']],\n",
    "                    [tile['bottomright_y'], tile['topleft_x']],\n",
    "                    [tile['topleft_y'], tile['topleft_x']]\n",
    "                ]\n",
    "                \n",
    "                # Determine color based on F1 score\n",
    "                f1 = tile_metric['f1_score']\n",
    "                if f1 > 0:\n",
    "                    fill_color = colormap(f1)\n",
    "                    opacity = 0.6\n",
    "                else:\n",
    "                    fill_color = '#cccccc'\n",
    "                    opacity = 0.3\n",
    "                \n",
    "                # Create detailed popup with all metrics\n",
    "                popup_html = f\"\"\"\n",
    "                <div style=\"font-family: Arial; width: 350px;\">\n",
    "                    <h4 style=\"margin: 0 0 10px 0; color: #333;\">Tile {tile_metric['tile_id']}</h4>\n",
    "                    \n",
    "                    <div style=\"background: #f0f0f0; padding: 8px; margin: 5px 0; border-radius: 4px;\">\n",
    "                        <b>Location:</b><br>\n",
    "                        Position: ({tile_metric['xtile']}, {tile_metric['ytile']})<br>\n",
    "                        Center: ({tile_metric['lat']:.6f}, {tile_metric['lon']:.6f})\n",
    "                    </div>\n",
    "                    \n",
    "                    <div style=\"background: #e8f5e9; padding: 8px; margin: 5px 0; border-radius: 4px;\">\n",
    "                        <b>Performance Metrics:</b><br>\n",
    "                        F1 Score: <b>{tile_metric['f1_score']:.3f}</b><br>\n",
    "                        Precision: {tile_metric['precision']:.3f}<br>\n",
    "                        Recall: {tile_metric['recall']:.3f}<br>\n",
    "                        IoU: {tile_metric['iou']:.3f}\n",
    "                    </div>\n",
    "                    \n",
    "                    <div style=\"background: #fff3e0; padding: 8px; margin: 5px 0; border-radius: 4px;\">\n",
    "                        <b>Detection Counts:</b><br>\n",
    "                        ML Network: {tile_metric['ml_network_count']}<br>\n",
    "                        ML Polygons: {tile_metric['ml_polygon_count']}<br>\n",
    "                        OSM Paths: {tile_metric['osm_count']}\n",
    "                    </div>\n",
    "                    \n",
    "                    <div style=\"background: #e1f5fe; padding: 8px; margin: 5px 0; border-radius: 4px;\">\n",
    "                        <b>Length Metrics (m):</b><br>\n",
    "                        True Positives: {tile_metric['tp_length_m']:.1f}m<br>\n",
    "                        False Positives: {tile_metric['fp_length_m']:.1f}m<br>\n",
    "                        False Negatives: {tile_metric['fn_length_m']:.1f}m\n",
    "                    </div>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Add area composition if available\n",
    "                if 'buildings_pct' in tile_metric:\n",
    "                    popup_html += f\"\"\"\n",
    "                    <div style=\"background: #f3e5f5; padding: 8px; margin: 5px 0; border-radius: 4px;\">\n",
    "                        <b>Area Composition:</b><br>\n",
    "                        Buildings: {tile_metric['buildings_pct']:.1f}%<br>\n",
    "                        Roads: {tile_metric['roads_pct']:.1f}%<br>\n",
    "                        Pedestrian Areas: {tile_metric['pedestrian_areas_pct']:.1f}%<br>\n",
    "                        Green Spaces: {tile_metric['green_spaces_pct']:.1f}%<br>\n",
    "                        Water: {tile_metric['water_pct']:.1f}%<br>\n",
    "                        Railways: {tile_metric['railways_pct']:.1f}%<br>\n",
    "                        Parking: {tile_metric['parking_pct']:.1f}%<br>\n",
    "                        Unmapped: {tile_metric['unmapped_pct']:.1f}%\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                \n",
    "                popup_html += \"</div>\"\n",
    "                \n",
    "                # Create polygon for tile\n",
    "                folium.Polygon(\n",
    "                    locations=tile_coords,\n",
    "                    color='black',\n",
    "                    weight=1,\n",
    "                    fill=True,\n",
    "                    fill_color=fill_color,\n",
    "                    fill_opacity=opacity,\n",
    "                    popup=folium.Popup(popup_html, max_width=400),\n",
    "                    tooltip=f\"Tile {tile_metric['tile_id']}: F1={f1:.3f}\"\n",
    "                ).add_to(tile_map)\n",
    "        \n",
    "        # Add study area boundary\n",
    "        bbox_coords = [\n",
    "            [tile_info['bbox'][0], tile_info['bbox'][1]],\n",
    "            [tile_info['bbox'][0], tile_info['bbox'][3]],\n",
    "            [tile_info['bbox'][2], tile_info['bbox'][3]],\n",
    "            [tile_info['bbox'][2], tile_info['bbox'][1]],\n",
    "            [tile_info['bbox'][0], tile_info['bbox'][1]]\n",
    "        ]\n",
    "        folium.PolyLine(\n",
    "            bbox_coords, \n",
    "            color='black', \n",
    "            weight=3, \n",
    "            opacity=0.8,\n",
    "            popup='Study Area Boundary'\n",
    "        ).add_to(tile_map)\n",
    "        \n",
    "        print(\"✓ Created interactive tile visualization\")\n",
    "        print(f\"  - {len(per_tile_metrics)} tiles displayed\")\n",
    "        print(f\"  - Color scale: F1 {min_f1:.3f} (red) to {max_f1:.3f} (green)\")\n",
    "        print(f\"  - Click on any tile to see detailed metrics\\n\")\n",
    "        \n",
    "        # Display the map\n",
    "        display(tile_map)\n",
    "    else:\n",
    "        print(\"⚠️ No tiles with valid F1 scores to visualize\")\n",
    "else:\n",
    "    print(\"⚠️ Per-tile metrics or tiles dataframe not available for visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
